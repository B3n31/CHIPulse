#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import csv
import requests
import re
from bs4 import BeautifulSoup, Tag, NavigableString

# ---------------- Configuration ----------------
OUTPUT_CSV = os.path.join("data", "raw", "committee_members.csv")
START_YEAR = 2011
END_YEAR   = 2025
USER_AGENT = "Mozilla/5.0 (compatible; Bot/0.1; +https://your.site/)"

def fetch_url(url):
    headers = {"User-Agent": USER_AGENT}
    r = requests.get(url, headers=headers, timeout=10)
    r.raise_for_status()
    return r.text

def scrape_chi_2011():
    url = "http://chi2011.org/authors/selecting-subcommittee.html"
    html = fetch_url(url)
    soup = BeautifulSoup(html, "html.parser")

    # å®šä½åˆ°çœŸæ­£çš„ä¸»å†…å®¹åŒº
    container = soup.select_one("div#main-text-single-col")
    if not container:
        print("[WARN] CHI 2011: æ‰¾ä¸åˆ°ä¸»ä½“åŒºåŸŸ")
        return []

    results = []
    # æ¯ä¸ªç‰ˆå—çš„æ ‡é¢˜æ˜¯ <h5><span id="...">ç‰ˆå—åç§°</span></h5>
    for h5 in container.find_all("h5"):
        span = h5.find("span", id=True)
        if not span:
            continue
        sub_name = span.get_text(strip=True)

        # è·³è¿‡æ‰€æœ‰ç©ºè¡Œï¼Œç›´æ¥æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ…å« â€œAssociate Chairsâ€ å­—æ ·çš„ <p>
        ac_p = h5.find_next_sibling(lambda t: isinstance(t, Tag)
                                           and t.name == "p"
                                           and "Associate Chairs" in t.get_text())
        if not ac_p:
            continue

        # åªå–è¿™ä¸ª <p> ä¸­ç›´æ¥å­èŠ‚ç‚¹çš„ <a>ï¼ˆä¸è·¨è¿‡å…¶ä»–å±‚çº§ï¼‰
        for a in ac_p.find_all("a", recursive=False):
            name = a.get_text(strip=True)
            if name:
                results.append((2011, sub_name, name))

    print(f"[OK]   CHI 2011: got {len(results)} rows")
    return results



def scrape_chi_2012():
    url = "https://chi2012.acm.org/cfp-selecting-subcommittee.shtml"
    try:
        html = fetch_url(url)
    except Exception as e:
        print(f"[WARN] CHI 2012: æ— æ³•è®¿é—®é¡µé¢ ({e})ï¼Œè·³è¿‡")
        return []
    
    soup = BeautifulSoup(html, "html.parser")
    container = soup.select_one("div.container div.content#content")
    if not container:
        print("[WARN] CHI 2012: æ‰¾ä¸åˆ°ä¸»ä½“åŒºåŸŸï¼Œè¯·æ£€æŸ¥ selector")
        return []
    
    results = []
    skip_ids = {"introduction", "guidance", "list-of-the-subcommittees"}
    for h2 in container.find_all("h2", id=True):
        if h2["id"] in skip_ids:
            continue
        sub_name = h2.get_text(strip=True)
        
        # å®šä½åˆ° Associate Chairs:
        ac_strong = h2.find_next(
            lambda t: isinstance(t, Tag)
                      and t.name == "strong"
                      and "associate chairs" in t.get_text(strip=True).lower()
        )
        if not ac_strong:
            continue
        
        # æ•´ä¸ª <p> é‡Œæ—¢æœ‰ Chairs: éƒ¨åˆ†ï¼Œä¹Ÿæœ‰ Associate Chairs: éƒ¨åˆ†
        parent_p = ac_strong.find_parent("p")
        
        # åªä» Associate Chairs: ä¹‹åå¼€å§‹å–
        in_ac_list = False
        for node in parent_p.contents:
            # å¦‚æœåˆç¢°åˆ° <strong>ï¼Œæ£€æŸ¥ä¸€ä¸‹æ˜¯ Chairs è¿˜æ˜¯ Associate Chairs
            if isinstance(node, Tag) and node.name == "strong":
                txt = node.get_text(strip=True).lower()
                in_ac_list = txt.startswith("associate chairs")
                continue
            
            if not in_ac_list:
                # è¿˜æ²¡åˆ°å‰¯ä¸»å¸­é‚£ä¸€èŠ‚ï¼Œå°±è·³è¿‡
                continue
            
            # åªå¤„ç†æ–‡æœ¬èŠ‚ç‚¹ï¼Œè·³è¿‡ <br>ã€<strong> ç­‰
            if isinstance(node, NavigableString):
                name = node.strip().strip('â€œâ€" ,')
                if name:
                    results.append((2012, sub_name, name))
        
    print(f"[OK]   CHIÂ 2012: got {len(results)} rows")
    return results



def scrape_chi_2013():
    url = ("https://chi2013.acm.org/authors/call-for-participation/papers-notes/selecting-a-subcommittee")
    html = fetch_url(url)
    soup = BeautifulSoup(html, "html.parser")

    # 1. å®šä½æ­£æ–‡å®¹å™¨
    entry = soup.select_one("div#post-316 .entry-content")
    if not entry:
        print("[WARN] CHIÂ 2013: æ‰¾ä¸åˆ° entry-content")
        return []

    # 2. å®šä½åˆ° â€œSubcommittee membershipâ€ æ®µ
    h2 = entry.find("h2", string=re.compile(r"Subcommittee membership", re.I))
    if not h2:
        print("[WARN] CHIÂ 2013: æ‰¾ä¸åˆ° Subcommittee membership æ ‡é¢˜")
        return []

    results = []
    sib = h2

    # 3. ä»è¿™ä¸ª H2 å¼€å§‹ï¼Œä¾æ¬¡æŸ¥æ‰¾åé¢çš„ <h3> + <table>
    while True:
        sib = sib.find_next_sibling()
        if not sib:
            break

        # åªå¤„ç† <h3> ä½œä¸ºå°èŠ‚
        if sib.name != "h3":
            continue

        full_title = sib.get_text(strip=True)
        # å»æ‰å°¾éƒ¨ â€œsubcommitteeâ€
        sub_name = re.sub(r"\s*subcommittee$", "", full_title, flags=re.I)

        # 4. æ‰¾åˆ°ç´§è·Ÿçš„é‚£å¼ è¡¨
        tbl = sib.find_next_sibling("table", class_="tableizer-table")
        if not tbl:
            continue

        # 5. éå†è¡¨æ ¼æ¯ä¸€è¡Œï¼ˆè·³è¿‡é¦–è¡Œè¡¨å¤´ï¼‰
        rows = tbl.select("tbody tr")[1:]
        for tr in rows:
            tds = tr.find_all("td")
            if len(tds) < 3:
                continue
            name = tds[0].get_text(strip=True)
            position = tds[2].get_text(strip=True)
            # åªä¿ç•™ Associate Chair (AC)
            if re.fullmatch(r"AC", position, re.I):
                results.append((2013, sub_name, name))

    print(f"[OK]   CHIÂ 2013: got {len(results)} rows")
    return results


def scrape_chi_2014to2015(year):
    # 2014/2015 çš„ URL éƒ½ä¸è¦æœ«å°¾æ–œæ 
    url = f"https://chi{year}.acm.org/authors/selecting-a-subcommittee"
    try:
        html = fetch_url(url)
    except Exception as e:
        print(f"[WARN] CHI {year}: æ— æ³•è®¿é—®é¡µé¢ ({e})ï¼Œè·³è¿‡")
        return []

    soup = BeautifulSoup(html, "html.parser")
    # 2014 ç”¨ #content-canvasï¼Œ2015 ç”¨ .container .column-main
    if year == 2014:
        container = soup.select_one("div#content-canvas")
    else:
        container = soup.select_one("div.container div.column-main")
    if not container:
        print(f"[WARN] CHI {year}: æ‰¾ä¸åˆ°ä¸»å†…å®¹åŒºï¼Œè¯·æ£€æŸ¥ selector")
        return []

    results = []
    # æ¯ä¸ªå°èŠ‚ç”± <h4 id="..."> åˆ†å‰²
    for h4 in container.find_all("h4", id=True):
        sub_name = h4.get_text(strip=True)

        # æ‰¾åˆ°ç´§è·Ÿçš„â€œSubcommittee:â€é‚£è¡Œ
        sc_strong = h4.find_next(
            lambda t: isinstance(t, Tag)
                      and t.name == "strong"
                      and "subcommittee" in t.get_text(strip=True).lower()
        )
        if not sc_strong:
            continue

        # ä»è¿™ä¸ª <strong> æ‰€åœ¨çš„ <p>ï¼Œå–å®ƒåé¢çš„ç¬¬ä¸€ä¸ªå…„å¼Ÿ <p>
        sc_p = sc_strong.find_parent("p").find_next_sibling("p")
        if not sc_p:
            continue

        # è¿™ä¸ª <p> é‡Œçš„å†…å®¹æ˜¯è‹¥å¹²ä¸ªæ–‡æœ¬èŠ‚ç‚¹/å¼•å·ï¼Œå†é…åˆ <br> åˆ†æ®µ
        buf = []
        def flush_buf():
            name = "".join(buf).strip().strip('â€œâ€"')
            if name:
                results.append((year, sub_name, name))
        for node in sc_p.children:
            if isinstance(node, Tag) and node.name == "br":
                flush_buf()
                buf = []
            else:
                text = ""
                if isinstance(node, NavigableString):
                    text = node.strip()
                elif isinstance(node, Tag):
                    text = node.get_text(strip=True)
                buf.append(text)
        # æœ€åä¸€æ®µ
        flush_buf()

    print(f"[OK]   CHI {year}: got {len(results)} rows")
    return results







def scrape_chi_2016():
    url = "https://chi2016.acm.org/wp/guide-to-selecting-a-subcommittee-for-submission/"
    try:
        html = fetch_url(url)
    except Exception:
        print("[WARN] CHI 2016: è®¿é—®å¤±è´¥æˆ–é¡µé¢ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return []

    soup = BeautifulSoup(html, "html.parser")
    container = soup.select_one("div.single_inside_content")
    if not container:
        print("[WARN] CHI 2016: æ‰¾ä¸åˆ°å†…å®¹å®¹å™¨ï¼Œè¯·ç¡®è®¤ selector")
        return []

    results = []
    skip_titles = {
        "list of the subcommittees",
        "overview",
        "subcommittee selection process",
    }

    for h2 in container.find_all("h2"):
        title = h2.get_text(strip=True)
        if title.lower() in skip_titles:
            continue
        sub_name = title

        # æ‰¾åˆ° â€œAssociate Chairs:â€ é‚£è¡Œ
        ac_tag = None
        for sib in h2.next_siblings:
            if isinstance(sib, Tag) and sib.name == "h2":
                break
            if (isinstance(sib, Tag)
                and sib.name == "p"
                and sib.find("strong")
                and "associate chairs" in sib.get_text(strip=True).lower()):
                ac_tag = sib
                break
        if not ac_tag:
            continue

        # æ‹¿åˆ°ç´§è·Ÿçš„é‚£ä¸ª <p>ï¼Œå®ƒå†…é‡Œé€šè¿‡ <a> + <br> æˆ–çº¯æ–‡æœ¬è®°å½•æ‰€æœ‰æˆå‘˜
        members_p = ac_tag.find_next_sibling(lambda t: isinstance(t, Tag) and t.name=="p")
        if not members_p:
            continue

        # æˆ‘ä»¬å…ˆæŠŠæ•´æ®µç”¨ <br> æ‹†å¼€
        segments = []
        buf = []
        for node in members_p.children:
            if isinstance(node, Tag) and node.name == "br":
                # ä¸€æ®µç»“æŸ
                segment = "".join(buf).strip()
                if segment:
                    segments.append(segment)
                buf = []
            else:
                # æ–‡æœ¬èŠ‚ç‚¹æˆ– <a>
                text = ""
                if isinstance(node, NavigableString):
                    text = node.strip()
                elif isinstance(node, Tag):
                    text = node.get_text(strip=True)
                buf.append(text)
        # æœ€åä¸€æ®µ
        last = "".join(buf).strip()
        if last:
            segments.append(last)

        # è¿‡æ»¤ç©ºé¡¹ & â€œAssociate Chairs:â€ ç­‰
        for seg in segments:
            seg = seg.strip(' ,\n')
            if not seg:
                continue
            # æœ‰æ—¶å€™ä¼šå‡ºç°å‰é¢å¤šä½™çš„é€—å·æˆ–â€œAssociate Chairsâ€
            clean = seg
            if clean.lower().startswith("associate chairs"):
                continue
            # æœ€ç»ˆåº”è¯¥å½¢å¦‚ "å§“å, å•ä½"
            results.append((2016, sub_name, clean))

    print(f"[OK]   CHI 2016: got {len(results)} rows")
    return results

def scrape_chi_2017():
    url = "https://chi2017.acm.org/select-subcommittee.html"
    try:
        html = fetch_url(url)
    except Exception:
        print("[WARN] CHI 2017: é¡µé¢è®¿é—®å¤±è´¥ï¼Œè·³è¿‡")
        return []

    soup = BeautifulSoup(html, "html.parser")
    results = []

    # ç”¨é”šç‚¹ <a class="myanchor" name="..."> æ¥åˆ’åˆ†å­ç‰ˆå—
    anchors = soup.find_all("a", class_="myanchor", attrs={"name": True})
    for anc in anchors:
        sub_name = anc["name"]  # slug å½¢å¼ï¼Œæ¯”å¦‚ "user-experience-and-usability"
        # åœ¨åŒä¸€ç‰ˆå—é‡Œï¼Œå…ˆæ‰¾é‚£è¡Œ Associate Chairs
        ac_p = None
        sib = anc
        while True:
            sib = sib.find_next_sibling()
            if not sib:
                break
            # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªå°èŠ‚çš„é”šç‚¹ï¼Œå°±ç»“æŸ
            if isinstance(sib, Tag) and sib.name=="a" and "myanchor" in sib.get("class", []):
                break
            # æ‰¾åˆ°æ ‡è®° Associate Chairs çš„ <p>
            if isinstance(sib, Tag) and sib.name=="p":
                span = sib.find("span", class_="MyBolding")
                if span and re.search(r"associate chairs", span.get_text(), re.I):
                    ac_p = sib
                    break
        if not ac_p:
            continue

        # ä» Associate Chairs é‚£è¡Œçš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹å¼€å§‹ï¼Œæ”¶é›†æ‰€æœ‰ â€œå§“å, å•ä½â€ å½¢å¼çš„ <p>
        sib2 = ac_p
        while True:
            sib2 = sib2.find_next_sibling()
            if not sib2: 
                break
            if isinstance(sib2, Tag):
                # ç¢°åˆ°æ–°çš„é”šç‚¹å°±ç»“æŸ
                if sib2.name=="a" and "myanchor" in sib2.get("class", []):
                    break
                if sib2.name=="p":
                    text = sib2.get_text(" ", strip=True)
                    # è·³è¿‡ç©ºè¡Œå’Œæ ‡é¢˜è¡Œ
                    if not text or re.search(r"associate chairs", text, re.I):
                        continue
                    # åªè¦åŒ…å«é€—å·ï¼Œå°±è®¤ä¸ºæ˜¯ â€œå§“å, å•ä½â€
                    if "," in text:
                        results.append((2017, sub_name, text))
        # ä¸‹ä¸€é”šç‚¹ç»§ç»­

    print(f"[OK]   CHI 2017: got {len(results)} rows")
    return results

def scrape_chi_2018to2020(year):
    if year == 2018:
        url = "https://chi2018.acm.org/papers/selecting-a-subcommittee/"
        post_id = "524"
    elif year == 2019:
        url = "https://chi2019.acm.org/papers/selecting-a-subcommittee/"
        post_id = "215"
    else:  # 2020
        url = "https://chi2020.acm.org/authors/papers/selecting-a-subcommittee/"
        post_id = "215"

    try:
        html = fetch_url(url)
    except:
        print(f"[WARN] CHI {year}: è®¿é—®å¤±è´¥ï¼Œè·³è¿‡")
        return []

    soup = BeautifulSoup(html, "html.parser")
    post = soup.select_one(f"div#post-{post_id}")
    if not post:
        print(f"[WARN] CHI {year}: æ‰¾ä¸åˆ° post-{post_id}")
        return []

    results = []
    for sub_h3 in post.find_all("h3", id=True):
        sub_name = sub_h3.get_text(strip=True)
        sib = sub_h3
        ac_block = None

        while True:
            sib = sib.find_next_sibling()
            if not sib or (sib.name=="h3" and sib.has_attr("id")):
                break
            # æ‰¾ <p><strong>Associate Chairs</strong></p> æˆ– <p><b>Associate Chairs</b></p>
            if sib.name=="p" and sib.find(lambda t: t.name in ("strong","b") and "associate chairs" in t.get_text(strip=True).lower()):
                ac_block = sib
                break

        if not ac_block:
            continue

        lst = ac_block.find_next_sibling(lambda t: t.name in ("ul","ol"))
        if not lst:
            continue

        for li in lst.find_all("li"):
            results.append((year, sub_name, li.get_text(" ", strip=True)))

    print(f"[OK]   CHI {year}: got {len(results)} rows")
    return results


def scrape_chi_2021():
    url = "https://chi2021.acm.org/for-authors/presenting/papers/selecting-a-subcommittee/"
    html = fetch_url(url)
    soup = BeautifulSoup(html, "html.parser")

    container = soup.select_one("div.post-entry")
    if not container:
        print("[WARN] CHI 2021: æ‰¾ä¸åˆ°ä¸»å†…å®¹åŒº")
        return []

    results = []
    # æ‰€æœ‰å¸¦ id çš„ <h4> éƒ½æ˜¯å„ subcommittee æ ‡é¢˜
    for sub_h4 in container.find_all("h4", id=True):
        sub_name = sub_h4.get_text(strip=True)
        # å¾€ä¸‹æ‰¾ç¬¬ä¸€ä¸ª â€œAssociate Chairsâ€ <h4>
        sib = sub_h4
        ac_h4 = None
        while True:
            sib = sib.find_next_sibling()
            if not sib or (sib.name=="h4" and sib.has_attr("id")):
                # æ’åˆ°ä¸‹ä¸€ä¸ª subcommittee æˆ–æ— æ›´å¤šèŠ‚ç‚¹ï¼Œå°±åœ
                break
            if sib.name=="h4" and "associate chairs" in sib.get_text(strip=True).lower():
                ac_h4 = sib
                break

        if not ac_h4:
            continue
        ul = ac_h4.find_next_sibling("ul")
        if not ul:
            continue

        for li in ul.find_all("li"):
            name = li.get_text(" ", strip=True)
            results.append((2021, sub_name, name))

    print(f"[OK]   CHI 2021: got {len(results)} rows")
    return results


def scrape_chi_2022():
    url = "https://chi2022.acm.org/subcommittees/selecting-a-subcommittee/"
    try:
        html = fetch_url(url)
    except Exception:
        print("[WARN] CHI 2022: åŸŸåè§£ææˆ–ç½‘ç»œé”™è¯¯ï¼Œè·³è¿‡")
        return []

    soup = BeautifulSoup(html, "html.parser")
    container = soup.select_one("div.entry-content")
    if not container:
        print("[WARN] CHI 2022: æ‰¾ä¸åˆ°ä¸»å†…å®¹åŒº")
        return []

    results = []
    for h3 in container.find_all("h3", id=True):
        sub = h3.get_text(strip=True)
        block = h3.find_next_sibling("div", class_="insert-page")
        if not block: continue

        ac = block.find(lambda t: t.name=="h4" 
                        and "associate chairs" in t.get_text(strip=True).lower())
        if not ac: continue

        ul = ac.find_next_sibling("ul")
        if not ul: continue

        for li in ul.find_all("li"):
            results.append((2022, sub, li.get_text(" ", strip=True)))

    print(f"[OK]   CHI 2022: got {len(results)} rows")
    return results


# ---------------- ä¸“é—¨å¤„ç† CHI 2023 ----------------
def scrape_chi_2023():
    url = "https://chi2023.acm.org/subcommittees/selecting-a-subcommittee/"
    html = fetch_url(url)
    soup = BeautifulSoup(html, "html.parser")

    container = soup.select_one("div.entry-content.clearfix")
    if not container:
        print("[WARN] CHI 2023: æ‰¾ä¸åˆ°ä¸»å†…å®¹åŒº")
        return []

    results = []
    for h2 in container.find_all("h2", id=True):
        sub_name = h2.get_text(strip=True)
        block = h2.find_next_sibling("div", class_="insert-page")
        if not block:
            continue

        # æ‰¾ â€œAssociate Chairsâ€ æ ‡é¢˜
        ac_h3 = block.find(
            lambda tag: tag.name=="h3"
                        and "associate chairs" in tag.get_text(strip=True).lower()
        )
        if not ac_h3:
            continue

        # ç´§è·Ÿçš„ <ul> é‡Œé¢çš„ <li> å°±æ˜¯æˆå‘˜
        ul = ac_h3.find_next_sibling("ul")
        if not ul:
            continue

        for li in ul.find_all("li"):
            name = li.get_text(" ", strip=True)
            results.append((2023, sub_name, name))

    print(f"[OK]   CHI 2023: got {len(results)} rows")
    return results


# ---------------- å¤„ç† CHI 2024+ ----------------
def scrape_chi_year(year):
    url = f"https://chi{year}.acm.org/subcommittees/selecting-a-subcommittee/"
    html = fetch_url(url)
    soup = BeautifulSoup(html, "html.parser")

    container = soup.select_one("div.entry-content.clearfix")
    if not container:
        print(f"[WARN] CHI {year}: æ— æ³•å®šä½ä¸»å®¹å™¨ï¼Œè·³è¿‡")
        return []

    results = []

    h2s = []
    for h2 in container.find_all("h2", id=True):
        t = h2.get_text(strip=True).lower()
        if any(skip in t for skip in (
            "overview","composition","selecting a subcommittee","list of the subcommittees"
        )):
            continue
        h2s.append(h2)

    if h2s:
        for h2 in h2s:
            cname = h2.get_text(strip=True)
            sib = h2
            while True:
                sib = sib.find_next_sibling()
                if not sib or sib.name == "h2":
                    break
                if sib.name == "h3":
                    h3_text = sib.get_text(strip=True).lower()
                    if h3_text.startswith("associate chairs"):
                        ul = sib.find_next("ul")
                        if not ul:
                            continue
                        for li in ul.find_all("li"):
                            name = li.get_text(" ", strip=True)
                            results.append((year, cname, name))
        print(f"[OK]   CHI {year}: got {len(results)} rows")
        return results

    print(f"[OK]   CHI {year}: æŠ“å–åˆ° {len(results)} æ¡ Associate Chairs")
    return results

# ---------------- å…¶ä»–é¡¶ä¼š ----------------
def scrape_uist_year(year):
    return []

def scrape_cscw_year(year):
    return []

# ---------------- Main ----------------
def main():
    os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)
    with open(OUTPUT_CSV, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(["year","venue","committee","member"])
        
        for yr in range(START_YEAR, END_YEAR+1):
            if yr == 2011:
                rows = scrape_chi_2011()
            elif yr == 2012:
                rows = scrape_chi_2012()
            elif yr == 2013:
                rows = scrape_chi_2013()
            elif yr == yr in (2014, 2015):
                rows = scrape_chi_2014to2015(yr)
            elif yr == 2016:
                rows = scrape_chi_2016()
            elif yr == 2017:
                rows = scrape_chi_2017()
            elif yr in (2018, 2019, 2020):
                rows = scrape_chi_2018to2020(yr)
            elif yr == 2021:
                rows = scrape_chi_2021()
            elif yr == 2021:
                rows = scrape_chi_2021()
            elif yr == 2022:
                rows = scrape_chi_2022()
            elif yr == 2023:
                rows = scrape_chi_2023()
            else:
                rows = scrape_chi_year(yr)

            for y, c, m in rows:
                w.writerow([y, "CHI", c, m])
            time.sleep(1)

    print("ğŸ‰ å®Œæˆï¼Œè¾“å‡ºåœ¨", OUTPUT_CSV)

if __name__ == "__main__":
    main()
